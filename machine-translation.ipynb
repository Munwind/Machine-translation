{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.678560Z","iopub.execute_input":"2025-02-20T13:35:38.678947Z","iopub.status.idle":"2025-02-20T13:35:38.683290Z","shell.execute_reply.started":"2025-02-20T13:35:38.678906Z","shell.execute_reply":"2025-02-20T13:35:38.682254Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"class InputEmbeddings(nn.Module):\n\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n\n        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.684486Z","iopub.execute_input":"2025-02-20T13:35:38.684742Z","iopub.status.idle":"2025-02-20T13:35:38.705406Z","shell.execute_reply.started":"2025-02-20T13:35:38.684721Z","shell.execute_reply":"2025-02-20T13:35:38.704553Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, seq_len, dropout):\n        super().__init__()\n\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n\n        # To get the positional encoding, we use sine ans cos function for each (i, pos)\n        # Let's build a matrix with the shape (seq_len, d_model)\n\n        pe = torch.zeros(seq_len, d_model)\n\n        # create a vector position with shape (seq_len, 1)\n        position = torch.arange(0, seq_len, dtype=float).unsqueeze(1) # shape (seq_len, 1) with values from 0 to seq_len - 1\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        # Apply the sin and cos\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0) # shape (1, seq_len, d_model)\n\n        self.register_buffer('pe', pe)  # make sure this value will not be updated during training\n\n    def forward(self, x):\n        x += (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n        return self.dropout(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.707237Z","iopub.execute_input":"2025-02-20T13:35:38.707447Z","iopub.status.idle":"2025-02-20T13:35:38.729897Z","shell.execute_reply.started":"2025-02-20T13:35:38.707429Z","shell.execute_reply":"2025-02-20T13:35:38.729056Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n\n    def __init__(self, epsilon = 1e-6):\n        super().__init__()\n\n        self.epsilon = epsilon\n        self.alpha = nn.Parameter(torch.ones(1))\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        mean = x.mean(dim = -1, keepdim = True)\n        std = x.std(dim = -1, keepdim = True)\n        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.731176Z","iopub.execute_input":"2025-02-20T13:35:38.731461Z","iopub.status.idle":"2025-02-20T13:35:38.745685Z","shell.execute_reply.started":"2025-02-20T13:35:38.731434Z","shell.execute_reply":"2025-02-20T13:35:38.745073Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"class FeedForward(nn.Module):\n\n    def __init__(self, d_model, d_ff, dropout):\n        super().__init__()\n\n        self.linear1 = nn.Linear(d_model, d_ff) # W1 and B1\n        self.linear2 = nn.Linear(d_ff, d_model) # W2 and B2\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.dropout(torch.relu(x))\n        x = self.linear2(x)\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.746316Z","iopub.execute_input":"2025-02-20T13:35:38.746501Z","iopub.status.idle":"2025-02-20T13:35:38.762684Z","shell.execute_reply.started":"2025-02-20T13:35:38.746478Z","shell.execute_reply":"2025-02-20T13:35:38.762075Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads, dropout):\n        super().__init__()\n\n        self.d_model = d_model\n        self.h = num_heads\n        self.d_k = d_model // num_heads\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n\n        self.w_o = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout):\n        d_k = query.shape[-1]\n\n        # [batch, h, seq_len, d_k ] -> [batch, h, seq_len, seq_len]\n        attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            attention_score.masked_fill_(mask == 0, -1e9)\n\n        attention_score = attention_score.softmax(dim = -1)     # [batch, seq_len, seq_len]\n\n        if dropout is not None:\n            attention_score = dropout(attention_score)\n\n        return (attention_score @ value), attention_score\n\n    def forward(self, Q, K, V, mask):\n        # shape of query, key, value : [batch_size, seq_len, d_model]\n        query = self.w_q(Q)\n        key = self.w_k(K)\n        value = self.w_v(V)\n\n        # [batch, seq_len, d_model]  -> [batch, seq_len, h, d_k] -> [batch, h, seq_len, d_k]\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n\n\n        x, self.attention_score = MultiheadAttention.attention(query, key, value, mask, self.dropout)\n\n        # Combine the head\n\n        # back to the shape [batch, seq_len, d_model]\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n        return self.w_o(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.763436Z","iopub.execute_input":"2025-02-20T13:35:38.763686Z","iopub.status.idle":"2025-02-20T13:35:38.783854Z","shell.execute_reply.started":"2025-02-20T13:35:38.763656Z","shell.execute_reply":"2025-02-20T13:35:38.783284Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n\n    def __init__(self, dropout):\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalization()\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.784600Z","iopub.execute_input":"2025-02-20T13:35:38.784903Z","iopub.status.idle":"2025-02-20T13:35:38.802631Z","shell.execute_reply.started":"2025-02-20T13:35:38.784874Z","shell.execute_reply":"2025-02-20T13:35:38.801792Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n\n    def __init__(self, self_attention_block: MultiheadAttention, feed_forward_block: FeedForward, dropout):\n        super().__init__()\n\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n\n    def forward(self, x, src_mask):\n        # the src_mask is to make sure that the padding words will have no connection with the words\n        x = self.residual_connections[0](x, lambda x : self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.850671Z","iopub.execute_input":"2025-02-20T13:35:38.850934Z","iopub.status.idle":"2025-02-20T13:35:38.856026Z","shell.execute_reply.started":"2025-02-20T13:35:38.850908Z","shell.execute_reply":"2025-02-20T13:35:38.855125Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n\n        self.layers = layers\n        self.norm = LayerNormalization()\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.857126Z","iopub.execute_input":"2025-02-20T13:35:38.857406Z","iopub.status.idle":"2025-02-20T13:35:38.871731Z","shell.execute_reply.started":"2025-02-20T13:35:38.857378Z","shell.execute_reply":"2025-02-20T13:35:38.870931Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n\n    def __init__(self, self_attention_block: MultiheadAttention, cross_attention_block: MultiheadAttention, feed_forward_block: FeedForward, dropout: float):\n        super().__init__()\n\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.dropout = nn.Dropout(dropout)\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](x, lambda x : self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x : self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        x = self.residual_connections[2](x, self.feed_forward_block)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.872795Z","iopub.execute_input":"2025-02-20T13:35:38.873057Z","iopub.status.idle":"2025-02-20T13:35:38.887304Z","shell.execute_reply.started":"2025-02-20T13:35:38.873030Z","shell.execute_reply":"2025-02-20T13:35:38.886506Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"class Decoder(nn.Module):\n\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n\n        self.layers = layers\n        self.norm = LayerNormalization()\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.888451Z","iopub.execute_input":"2025-02-20T13:35:38.888710Z","iopub.status.idle":"2025-02-20T13:35:38.910700Z","shell.execute_reply.started":"2025-02-20T13:35:38.888676Z","shell.execute_reply":"2025-02-20T13:35:38.909878Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"# The target is to make [batch, seq_len, d_model] -> [batch, seq_len, vocab_size]\n\nclass ProjectionLayer(nn.Module):\n\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n\n        self.proj = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        return torch.log_softmax(self.proj(x), dim = -1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.911397Z","iopub.execute_input":"2025-02-20T13:35:38.911586Z","iopub.status.idle":"2025-02-20T13:35:38.924682Z","shell.execute_reply.started":"2025-02-20T13:35:38.911569Z","shell.execute_reply":"2025-02-20T13:35:38.923936Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"class Transformers(nn.Module):\n\n    def __init__ (self, encoder : Encoder, decoder: Decoder, src_embedding: InputEmbeddings, tgt_embeddings: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embeddings = src_embedding\n        self.tgt_embeddings = tgt_embeddings\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.porjectionLayer = projection_layer\n\n    def encode(self, src, src_mask):\n        src = self.src_embeddings(src)\n        src = self.src_pos(src)\n\n        return self.encoder(src, src_mask)\n\n    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n        tgt = self.tgt_embeddings(tgt)\n        tgt = self.tgt_pos(tgt)\n\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n\n    def project(self, x):\n        return self.porjectionLayer(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.925427Z","iopub.execute_input":"2025-02-20T13:35:38.925691Z","iopub.status.idle":"2025-02-20T13:35:38.934735Z","shell.execute_reply.started":"2025-02-20T13:35:38.925664Z","shell.execute_reply":"2025-02-20T13:35:38.933890Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"def build_transformers(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, d_model = 64, N = 3, num_heads = 8, dropout = 0.1, d_ff = 1024):\n    # get the embeddings first\n    src_embeddings = InputEmbeddings(d_model, src_vocab_size)\n    tgt_embeddings = InputEmbeddings(d_model, tgt_vocab_size)\n\n    # get the positional encoding\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n\n    # get the encoder\n    encoder_blocks = []\n    for _ in range(N):\n        encoder_self_attention_block = MultiheadAttention(d_model, num_heads, dropout)\n        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    # get the decoder\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = MultiheadAttention(d_model, num_heads, dropout)\n        decoder_cross_attention_block = MultiheadAttention(d_model, num_heads, dropout)\n        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n\n    encoder = Encoder(nn.ModuleList(encoder_blocks))\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n\n    # get the linear layer\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n\n    transformers = Transformers(encoder, decoder, src_embeddings, tgt_embeddings, src_pos, tgt_pos, projection_layer)\n\n    # initialize the parameters\n    for p in transformers.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n\n    return transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.935567Z","iopub.execute_input":"2025-02-20T13:35:38.935811Z","iopub.status.idle":"2025-02-20T13:35:38.955985Z","shell.execute_reply.started":"2025-02-20T13:35:38.935780Z","shell.execute_reply":"2025-02-20T13:35:38.955257Z"}},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":"1. # Get data from huggingface ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nfrom pathlib import Path\n\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\ndef get_all_sentences(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]\n\ndef get_tokenizers(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens = ['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_freuency = 2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n\n    return tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.958257Z","iopub.execute_input":"2025-02-20T13:35:38.958498Z","iopub.status.idle":"2025-02-20T13:35:38.972922Z","shell.execute_reply.started":"2025-02-20T13:35:38.958478Z","shell.execute_reply":"2025-02-20T13:35:38.972059Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n\n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, index):\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n\n        # add sos and eos for the src\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype= torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype= torch.int64)\n            ]\n        )\n\n        # add sos to the decoder input\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype= torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype= torch.int64)\n            ]\n        )\n\n        # add pos to the label (what we expect from the ouput of the decoder)\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype= torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype= torch.int64)\n            ]\n        )\n\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return{\n            \"encoder_input\": encoder_input, # seq_len\n            \"decoder_input\": decoder_input, # seq_len\n            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),   # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),   # (1, 1, seq_len) & (1, seq_len)\n            \"label\": label,\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.973919Z","iopub.execute_input":"2025-02-20T13:35:38.974139Z","iopub.status.idle":"2025-02-20T13:35:38.988533Z","shell.execute_reply.started":"2025-02-20T13:35:38.974116Z","shell.execute_reply":"2025-02-20T13:35:38.987785Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"def casual_mask(size):\n    mask = torch.triu(torch.ones(1, size, size), diagonal= 1).type(torch.int)\n    return mask == 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:38.989265Z","iopub.execute_input":"2025-02-20T13:35:38.989512Z","iopub.status.idle":"2025-02-20T13:35:39.010552Z","shell.execute_reply.started":"2025-02-20T13:35:38.989484Z","shell.execute_reply":"2025-02-20T13:35:39.009922Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"def get_ds(config):\n    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n\n    # Build tokenizers\n    tokenizer_src = get_tokenizers(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = get_tokenizers(config, ds_raw, config['lang_tgt'])\n\n    # 90% for training and 10% for validation\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    max_len_src = 0\n    max_len_tgt = 0\n\n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_src.encode(item['translation'][config['lang_tgt']]).ids\n\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f'Max length of source sentence: {max_len_src}, target: {max_len_tgt}')\n\n    train_data_loader = DataLoader(train_ds, batch_size= config['batch_size'], shuffle= True)\n    val_dataloader = DataLoader(val_ds, batch_size= 1, shuffle= True)\n\n    return train_data_loader, val_dataloader, tokenizer_src, tokenizer_tgt\n\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformers(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n\n    return model\n\ndef get_config():\n    return {\n        \"batch_size\": 32,\n        \"num_epoch\": 10,\n        \"lr\": 1e-4,\n        \"seq_len\": 350,\n        \"d_model\": 64,\n        \"lang_src\": \"en\",\n        \"lang_tgt\": \"it\",\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\",\n        \"preload\": None,\n        \"tokenizer_file\": \"tokenizer_{0}.json\",\n        \"experiment_name\": \"runs/tmodel\"\n    }\n\ndef get_weights_file_path(config, epoch):\n    model_folder = config['model_folder']\n    model_basename = config['model_basename']\n    model_filename = f'{model_basename}{epoch}.pt'\n\n    return str(Path('.') / model_folder / model_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:39.011169Z","iopub.execute_input":"2025-02-20T13:35:39.011390Z","iopub.status.idle":"2025-02-20T13:35:39.021430Z","shell.execute_reply.started":"2025-02-20T13:35:39.011371Z","shell.execute_reply":"2025-02-20T13:35:39.020787Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    # precompute the encoder output and reuse for every token we get from the decoder\n    encoder_output = model.encode(source, source_mask)\n\n    # initialize the decoder input with the sos token\n    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n\n        # build the mask for target (decoder input)\n        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        # calculate the output of the decoder\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        # get the next token\n        prob = model.project(out[:, -1])\n        # select the next word with the highes probability\n        _, next_word = torch.max(prob, dim= 1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim= 1)\n\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:39.022254Z","iopub.execute_input":"2025-02-20T13:35:39.022518Z","iopub.status.idle":"2025-02-20T13:35:39.042322Z","shell.execute_reply.started":"2025-02-20T13:35:39.022491Z","shell.execute_reply":"2025-02-20T13:35:39.041562Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"def run_validaton(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, num_examples = 2):\n    model.eval()\n    count = 0\n\n    console_width = 80\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch['encoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n\n            assert encoder_input.size(0) == 1       # Make sure the batch size = 1\n\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            source_text = batch['src_text'][0]\n            target_text = batch['tgt_text'][0]\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n\n            print_msg('-'*console_width)\n            print_msg(f'SOURCE: {source_text}')\n            print_msg(f'TARGET: {target_text}')\n            print_msg(f'PREDICTED: {model_out_text}')\n\n\n            if count == num_examples:\n                break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:39.043168Z","iopub.execute_input":"2025-02-20T13:35:39.043419Z","iopub.status.idle":"2025-02-20T13:35:39.058429Z","shell.execute_reply.started":"2025-02-20T13:35:39.043399Z","shell.execute_reply":"2025-02-20T13:35:39.057739Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_model(config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    print(f'Using device {device}')\n\n    Path(config['model_folder']).mkdir(parents= True, exist_ok= True)\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n\n    optimizer = optim.Adam(model.parameters(), lr = config['lr'], eps= 1e-9)\n\n    initial_epoch = 0\n    global_step = 0\n\n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename)\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index= tokenizer_src.token_to_id('[PAD]'), label_smoothing= 0.1)\n\n    for epoch in range(initial_epoch, config['num_epoch']):\n        batch_iterator = tqdm(train_dataloader, desc= f'Processing epoch {epoch : 02d}')\n\n        for batch in batch_iterator:\n            g.train()\n\n            encoder_input = batch['encoder_input'].to(device)   # [batch, seq_len]\n            decoder_input = batch['decoder_input'].to(device)   # [batch, seq_len]\n            encoder_mask = batch['encoder_mask'].to(device)     # [batch, 1, 1, seq_len]\n            decoder_mask = batch['decoder_mask'].to(device)     # [batch, 1, seq_len, seq_len]\n\n            encoder_output = model.encode(encoder_input, encoder_mask)  # (batch, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)    # [batch, seq_len, d_model]\n            proj_output = model.project(decoder_output)     # [batch, seq_len, tgt_vocab_size]\n\n            label = batch['label'].to(device)   # (batch, seq_len)\n\n            # (batch, seq_len, tgt_vocab_size) -> (batch * seq_len, tgt_vocab_size)\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n\n            batch_iterator.set_postfix({f'loss': f'{loss.item() : 6.3f}'})\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            global_step += 1\n\n\n        run_validaton(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n\n        # Save the model\n        model_filename = get_weights_file_path(config, f'{epoch : 02d}')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:39.059131Z","iopub.execute_input":"2025-02-20T13:35:39.059323Z","iopub.status.idle":"2025-02-20T13:35:39.078373Z","shell.execute_reply.started":"2025-02-20T13:35:39.059307Z","shell.execute_reply":"2025-02-20T13:35:39.077662Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"import warnings\nif __name__ == '__main__':\n    warnings.filterwarnings('ignore')\n    config = get_config()\n    train_model(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:35:39.078878Z","iopub.execute_input":"2025-02-20T13:35:39.079107Z","iopub.status.idle":"2025-02-20T14:32:13.746078Z","shell.execute_reply.started":"2025-02-20T13:35:39.079089Z","shell.execute_reply":"2025-02-20T14:32:13.745161Z"}},"outputs":[{"name":"stdout","text":"Using device cuda\nMax length of source sentence: 309, target: 274\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  0: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=7.323]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: Then they talked about horses, about that day's races, and how gallantly Vronsky's Atlasny had won the first prize.\nTARGET: Dopo, il discorso cadde sui cavalli, sulle corse di quel giorno e su come audacemente aveva vinto il primo premio Atlasnyj di Vronskij.\nPREDICTED: , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n--------------------------------------------------------------------------------\nSOURCE: I expect you had only just woke up.\nTARGET: Voi probabilmente vi eravate appena svegliata.\nPREDICTED: , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  1: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=6.818]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: Indeed, she had quite a long argument with the Lory, who at last turned sulky, and would only say, 'I am older than you, and must know better'; and this Alice would not allow without knowing how old it was, and, as the Lory positively refused to tell its age, there was no more to be said.\nTARGET: Discusse lungamente col Lori, ma tosto costui le mostrò un viso accigliato, dicendo perentoriamente: — Son più vecchio di te, perciò ne so più di te; — ma Alice non volle convenirne se prima non le avesse detto quanti anni aveva. Il Lori non volle dirglielo, e la loro conversazione fu troncata.\nPREDICTED: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —\n--------------------------------------------------------------------------------\nSOURCE: The arbour was an arch in the wall, lined with ivy; it contained a rustic seat.\nTARGET: La grotta era scavata nel muro e tutta rivestita d'ellera; vi era un sedile rustico.\nPREDICTED: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  2: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=6.155]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: 'P.S. – I enclose some money, which you may need for your expenses.'\nTARGET: P. S. — In questa lettera è accluso il denaro che potrà essere necessario per le vostre spese».\nPREDICTED: — E non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non .\n--------------------------------------------------------------------------------\nSOURCE: 'Nothing,' she answered, in the same cold quiet manner.\nTARGET: — Io, nulla — ella rispose con altrettanta freddezza e calma.\nPREDICTED: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — E — E — E — E — E — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — E — E — — — — — — — — E — E — — — — — — — — — — — E — E — — — — — — — — — E — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — E — E , — — E — E , — — — — E — E — E — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  3: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=6.366]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: So little do we see before us in the world, and so much reason have we to depend cheerfully upon the great Maker of the world, that He does not leave His creatures so absolutely destitute, but that in the worst circumstances they have always something to be thankful for, and sometimes are nearer deliverance than they imagine; nay, are even brought to their deliverance by the means by which they seem to be brought to their destruction.\nTARGET: Tanto poco vediamo dinanzi a noi su questa terra, e tanta ragione abbiamo di essere grati al signore dell’universo, perchè non lascia mai sì compiutamente derelitte le sue creature, che nelle condizioni anche più triste non abbiano alcun che per ringraziarlo e talvolta sieno più vicine di quanto se lo figurano al porto di loro salvezza; anzi di frequente sono condotte a questo porto da quelle circostanze medesime che pareano fatte per trascinarle alla disperazione.\nPREDICTED: E non era un ’ , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia , e la mia .\n--------------------------------------------------------------------------------\nSOURCE: In the beginning of his reign he attacked Granada, and this enterprise was the foundation of his dominions.\nTARGET: Lui nel principio del suo regno assaltò la Granata; e quella impresa fu il fondamento dello stato suo.\nPREDICTED: E la mia , e la mia , e la mia , e la mia .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  4: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=6.441]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: Then the noblemen gaily scrambled for their overcoats and all drove to the cathedral.\nTARGET: E dopo di questo, i nobili presero allegramente le pellicce e andarono tutti alla cattedrale.\nPREDICTED: Il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo .\n--------------------------------------------------------------------------------\nSOURCE: She felt her head with her hand. 'Yes, my hair was done, but I don't in the least remember when.'\nTARGET: Tastò la testa con la mano. — Sì, mi sono pettinata, ma quando, non lo ricordo assolutamente”.\nPREDICTED: E non non non non non era un ’ ic , ma non era un ’ ic , e non era un ’ era un ’ .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  5: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=5.919]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: \"Now I am sure you can get on your feet,\" he said--\"try.\"\nTARGET: — Ora sono certo, — disse, — che vi reggerete in piedi; provate.\nPREDICTED: — Ma non è un ’ è detto , — disse , — disse .\n--------------------------------------------------------------------------------\nSOURCE: Don't you believe me?\nTARGET: Non mi credi forse?\nPREDICTED: E non è ?\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  6: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=6.023]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: 'But what am I asking about?' he said to himself.\nTARGET: Si fece pensieroso, ma immediatamente si corresse.\nPREDICTED: — Non è un ’ è ? — disse .\n--------------------------------------------------------------------------------\nSOURCE: Her heart is always aching about somebody.\nTARGET: Perché le duole il cuore per ogni cosa.\nPREDICTED: E non era un ’ altra , ma non era stato .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  7: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=5.645]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: As to going home, shame opposed the best motions that offered to my thoughts, and it immediately occurred to me how I should be laughed at among the neighbours, and should be ashamed to see, not my father and mother only, but even everybody else; from whence I have since often observed, how incongruous and irrational the common temper of mankind is, especially of youth, to that reason which ought to guide them in such cases—viz. that they are not ashamed to sin, and yet are ashamed to repent; not ashamed of the action for which they ought justly to be esteemed fools, but are ashamed of the returning, which only can make them be esteemed wise men.\nTARGET: Sul tornare a casa, la vergogna rintuzzava sempre quanti migliori pensieri potessero nascermi in mente, perchè la prima idea ad occorrermi, era quella della derisione che avrei trovata fra i miei concittadini, onde arrossiva non solamente di rivedere mio padre e mia madre, ma qualunque altra persona. Da quel momento ho fatto più volte una considerazione: come, cioè, sia fallace ed assurda in generale l’indole umana nell’istituire quei raziocini che dovrebbero guidarci in simili casi; non si ha vergogna della colpa, ma bensì del pentimento; non ci vergogniamo di un’azione che ne merita giustamente il credito di stolti, ma di un ravvedimento che solo potrebbe meritarci il nome di saggi.\nPREDICTED: E che non mi , che non si , e non si , e non si , e di un ’ altra , e di un ’ altra , e di , e di un ’ altra , che non si , che non si , e di un ’ altra , che non si , e di me , e di un ’ altra , e di , che non si , che non si , che non mi , che non , e , e di , e di un ’ altra , e di un ’ altra , e a un ’ altra , e di un ’ altra , e di un ’ altra , e di un ’ altra , e di un ’ altra , e di un ’ altra , e di un ’ altra , e di un ’ altra , e di un ’ altra , e , e , e , e di un ’ altra , e di , e di , e di , non era stato , e , non era stato , non era stato , e di , non era stato , e di cui era stato , che non era stato , non era stato , non era più che non era più che non era fatto che non era fatto che non era fatto che non era fatto che non era fatto che non era più mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio suo mio suo mio suo mio Rochester , e , e , e , e di mia Rochester , e , e , e si era , e , e , e\n--------------------------------------------------------------------------------\nSOURCE: 'I am not at all in the wrong toward her,' he thought. 'If she wants to punish herself, tant pis pour elle!'\nTARGET: “Io non sono colpevole in nulla verso di lei — egli pensava. — Se vuole punirsi, tant pis pour elle”.\nPREDICTED: — Ma non è un ’ è detto — disse Levin , — disse , e non è un ’ ic , e non è un ’ ic .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  8: 100%|██████████| 910/910 [05:37<00:00,  2.70it/s, loss=5.840]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: 'It is fine, is it not?\nTARGET: — Non è vero che è bella?\nPREDICTED: — Non è che non è ?\n--------------------------------------------------------------------------------\nSOURCE: But do take a seat.'\nTARGET: Ma sedetevi.\nPREDICTED: Ma non è un ’ altra .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch  9: 100%|██████████| 910/910 [05:37<00:00,  2.69it/s, loss=6.213]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: 'I quite agree with you,' said the Duchess; 'and the moral of that is--\"Be what you would seem to be\"--or if you'd like it put more simply--\"Never imagine yourself not to be otherwise than what it might appear to others that what you were or might have been was not otherwise than what you had been would have appeared to them to be otherwise.\"'\nTARGET: — Proprio così, — disse la Duchessa, — e la morale è questa: “Sii ciò che vuoi parere” o, se vuoi che te la dica più semplicemente: “Non credere mai d'essere diversa da quella che appari agli altri di esser o d'esser stata, o che tu possa essere, e l'essere non è altro che l'essere di quell'essere ch'è l'essere dell'essere, e non diversamente.”\nPREDICTED: — Non è che non ho detto che non ho detto , — disse , — disse , — disse , ma non è un ’ altra cosa che non è un ’ altra cosa che non è stato più di me , e che non è che non è stato più più più di me , ma non mi , e non è che non mi , e non è un ’ altra cosa non mi , che non mi , che non mi .\n--------------------------------------------------------------------------------\nSOURCE: While they were talking about their mutual Petersburg acquaintances Anna rose suddenly.\nTARGET: Parlando di conoscenti comuni di Pietroburgo, Anna si era alzata, svelta.\nPREDICTED: Il signor Rochester si era in un ’ ic , e la sua mano di Levin .\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}